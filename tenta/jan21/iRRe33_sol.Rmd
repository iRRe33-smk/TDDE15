---
title: "iRRe_sol"
author: "Isak Berntsson"
date: "10/25/2022"
output: pdf_document
---


# Task1
```{r}
library("bnlearn")
data("lizards")
```


```{r}

alpha = 0.05
print(head(lizards))

combs = t(combn(colnames(lizards),2))


#step1 

#S,H
res = ci.test(lizards$Species, lizards$Height, test="x2" )# < 0.05 # keep this arc
print(res)
#S,D
res = ci.test(lizards$Species, lizards$Diameter, test="x2" )# < 0.05 # keep this arc
print(res)
#H,D
res = ci.test(lizards$Height, lizards$Diameter, test="x2" )# > 0.05 # remove this arc
print(res)


# arcs = H-D, S-D

#step2
#orient arcs H-D and S-D
# H->D, S->D creating

#step3
#we have no undirected edges

net = empty.graph(colnames(lizards))


arcs = matrix(nrow=2,ncol=2)
arcs[1,] = c("Height","Diameter")
arcs[2,] = c("Species","Diameter")
net$arcs = arcs

plot(net)



```











# Task 2 HMM

```{r}

transition = matrix(0, 10, 10)
emission = matrix(0, 10, 10)
for (i in 0:9)
{
# Set transition values to .5 for S(current) and S(current+1)
transition[i+1, i:(i+1)%%10 +1] = 0.5
# Set emission values to .2 for S(current) and the four closest columns
emission[i+1, (i-2):(i+2)%%10 +1] = 0.2
}
transition
emission

```

```{r}
library(gRain)
#Tmax = 3

#state in time n, emission in time n
nodes = c("S0","S1", "S2","S3", "E0", "E1", "E2", "E3")
oneToTen = c("1","2","3","4","5","6","7","8","9","10")
net = empty.graph(nodes)

arcs = matrix(nrow = 0, ncol=2)

arcs = rbind(arcs, c("S0","E0"))
arcs = rbind(arcs, c("S1","E1"))
arcs = rbind(arcs, c("S2","E2"))
arcs = rbind(arcs, c("S3","E3"))

arcs = rbind(arcs, c("S0","S1"))
arcs = rbind(arcs, c("S1","S2"))
arcs = rbind(arcs, c("S2","S3"))

net$arcs = arcs
plot(net)


cptS0 = rep(1/10,10)
dim(cptS0) = c(10)
dimnames(cptS0) = list("S0"=oneToTen)

cptS1 = transition
dim(cptS1) = c(10,10)
dimnames(cptS1) = list("S1" = oneToTen, "S0"=oneToTen)

cptS2 = transition
dim(cptS2) = c(10,10)
dimnames(cptS2) = list("S2" = oneToTen, "S1"=oneToTen)

cptS3 = transition
dim(cptS3) = c(10,10)
dimnames(cptS3) = list("S3" = oneToTen, "S2"=oneToTen)



cptE0 = emission
dim(cptE0) = c(10,10)
dimnames(cptE0) = list("E0"=oneToTen,"S0"=oneToTen)

cptE1 = emission
dim(cptE1) = c(10,10)
dimnames(cptE1) = list("E1"=oneToTen,"S1"=oneToTen)

cptE2 = emission
dim(cptE2) = c(10,10)
dimnames(cptE2) = list("E2"=oneToTen,"S2"=oneToTen)

cptE3 = emission
dim(cptE3) = c(10,10)
dimnames(cptE3) = list("E3"=oneToTen,"S3"=oneToTen)



params = list("S0" = cptS0,"S1" = cptS1, "S2"= cptS2 ,"S3" = cptS3, "E0"= cptE0, "E1" = cptE1, "E2" = cptE2 , "E3" = cptE3)

fitted = custom.fit(net ,params)

grainNet = compile(as.grain(fitted))


#grainNet = setEvidence(grainNet,c("E0", "E2"), c("1","3"))
querygrain(setEvidence(grainNet,c("E0", "E2"), c("1","3")), nodes = c("S0"))
querygrain(setEvidence(grainNet,c("E0", "E2"), c("1","3")), nodes = c("S1"))
querygrain(setEvidence(grainNet,c("E0", "E2"), c("1","3")), nodes = c("S2"))
querygrain(setEvidence(grainNet,c("E0", "E2"), c("1","3")), nodes = c("S3"))


```







# Task 3


```{r}
#setwd("C:/Users/IsakG/projects/TDDE15/tenta/jan21")
#source("given.R")
GreedyPolicy <- function(x, y){
# Get a greedy action for state (x,y) from q_table.
#
# Args:
# x, y: state coordinates.
# q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
#
# Returns:
# An action, i.e. integer in {1,2,3,4}.
# Your code here
#
  
  equalsMax = which(q_table[x,y,]==max(q_table[x,y,]))
  if(length(equalsMax>1)){
    return(sample(equalsMax,1))
  }else{
    return(equalsMax[1])
  }
                  
}
EpsilonGreedyPolicy <- function(x, y, epsilon){
# Get an epsilon-greedy action for state (x,y) from q_table.
#
# Args:
# x, y: state coordinates.
# epsilon: probability of acting randomly.
#
# Returns:
# An action, i.e. integer in {1,2,3,4}.
# Your code here
  if (runif(1) > epsilon){
  
    return (GreedyPolicy(x, y))

  }else{
    return (sample(c(1, 2, 3, 4), 1))
  }
}
#Implementation of the q_learning function
q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95,
  beta = 0){
  # Perform one episode of Q-learning. The agent should move around in the
  # environment using the given transition model and update the Q-table.
  # The episode ends when the agent reaches a terminal state.
  #
  # Args:
  # start_state: array with two entries, describing the starting position of the agent.
  # epsilon (optional): probability of acting greedily.
  # alpha (optional): learning rate.
  # gamma (optional): discount factor.
  # beta (optional): slipping factor.
  # reward_map (global variable): a HxW array containing the reward given at each state.
  # q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  #
  # Returns:
  # reward: reward received in the episode.
  # correction: sum of the temporal difference correction terms over the episode.
  # q_table (global variable): Recall that R passes arguments by value. So, q_table being
  # a global variable can be modified with the superassigment operator <<-.
  # Your code here.
  episode_correction = 0
 " repeat{
    # Follow policy, execute action, get reward.
    policy = EpsilonGreedyPolicy(x=start_state[1], y=start_state[2], epsilon=epsilon)
    end_state = transition_model(x=start_state[1], y=start_state[2], action=policy, beta=beta)
    # Get reward
    reward = reward_map[end_state[1], end_state[2]]
    # Calculate temporal difference, store in sum, and update Q-table
    temporal_diff = reward + gamma * (max(q_table[end_state[1], end_state[2], ])) -q_table[start_state[1], start_state[2], policy]
    
    episode_correction = episode_correction + temporal_diff
    
    q_table[start_state[1], start_state[2], policy] <<- q_table[start_state[1], start_state[2], policy] + alpha*temporal_diff
    
    # Move agent
    start_state = end_state
    if(reward!=0){
    # End episode.
      return (c(reward,episode_correction))
    }
  }"
  n_sarsa = 2
  repeat{
    # Follow policy, execute action, get reward.
    
      
    policy_1 = EpsilonGreedyPolicy(x=start_state[1], y=start_state[2], epsilon=epsilon)
    end_state_1 = transition_model(x=start_state[1], y=start_state[2], action=policy, beta=beta)
    reward_1 = reward_map[end_state[1], end_state[2]]
    
    #if the first step is terminal do as before
    if(reward_1 != -1){
      temporal_diff = reward_1 + gamma * (q_table[end_state_1[1], end_state_1[2], policy_1 ]) - q_table[start_state[1], start_state[2], policy_1]
    
      episode_correction = episode_correction + temporal_diff
      
      q_table[start_state[1], start_state[2], policy] <<- q_table[start_state[1], start_state[2], policy] + alpha*temporal_diff
     
      return(c(reward,episode_correction))    
      
    }else{
      
      policy_2 = EpsilonGreedyPolicy(x=end_state_1[1], y=end_state_1[2], epsilon=epsilon)
      end_state_2 = transition_model(x=end_state[1], y=end_state[2], action=policy_2, beta=beta)
      reward_2 = reward_map[end_state_2[1], end_state_2[2]]
      
      if(reward_2 != -1){
        
        temporal_diff = reward_1 + gamma * (q_table[end_state_1[1], end_state_1[2], policy_1 ]) - q_table[start_state[1], start_state[2], policy_1]
    
        episode_correction = episode_correction + temporal_diff
        
        q_table[start_state[1], start_state[2], policy] <<- q_table[start_state[1], start_state[2], policy] + alpha*temporal_diff
        
        return(c(reward,episode_correction))    
      
        }
      
    }
    
    # Calculate temporal difference, store in sum, and update Q-table
    temporal_diff = reward_1 + gamma*reward_2 + gamma^2 * (max(q_table[end_state[1], end_state[2], ])) -q_table[start_state[1], start_state[2], policy]
    
    episode_correction = episode_correction + temporal_diff
    
    q_table[start_state[1], start_state[2], policy] <<- q_table[start_state[1], start_state[2], policy] + alpha*temporal_diff
    # Move agent
    start_state = end_state
    if(reward!=0){
    # End episode.
      return (c(reward,episode_correction))
    }
  }
}




```






```{r}
H <- 3
W <- 6

#changing_the vlaues
reward_map <- matrix(-1, nrow = H, ncol = W)
reward_map[1,2:5] <- -10
reward_map[1,6] <- 10

q_table <- array(0,dim = c(H,W,4))

vis_environment()

for(j in c(0,0.2,0.4,0.66)){
  q_table <- array(0,dim = c(H,W,4))
  
  for(i in 1:10000)
    foo <- q_learning(gamma = 0.6, beta = j, start_state = c(1,1))
  
  vis_environment(i, gamma = 0.6, beta = j)
}


```







